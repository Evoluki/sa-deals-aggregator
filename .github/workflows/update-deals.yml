# File: .github/workflows/update-deals.yml
name: Daily Deals Update

on:
  push:
    branches:
      - main
  schedule:
    # Runs at 04:00 UTC every day (≈ 06:00 SAST)
    - cron: '0 4 * * *'

jobs:
  update:
    runs-on: ubuntu-latest

    steps:
      # 1) Check out the repository
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      # 2) Set up Python 3.12
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      # 3) Install system packages needed to build lxml and run Chromium headless
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libxml2-dev libxslt-dev python3-dev \
            libnss3 libnspr4 libatk1.0-0 libatk-bridge2.0-0 \
            libcups2 libdrm2 libxkbcommon0 libxcomposite1 \
            libxdamage1 libxfixes3 libxrandr2 libgbm1 \
            libasound2-dev

      # 4) Create virtualenv & install Python deps
      - name: Create venv & install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate

          pip install --upgrade pip
          pip install wheel
          pip install -r requirements.txt

          # Pull in Playwright’s own Chromium + runtime libs
          playwright install chromium
          playwright install-deps chromium

      # 5) Run Takealot scraper (produces takealot_debug.html if zero deals)
      - name: Run Takealot scraper
        run: |
          source venv/bin/activate
          python scraper_takealot_sqlite.py || true

          if [ -f takealot_debug.html ]; then
            echo "⚠️  takealot_debug.html exists → inspect via artifact."
          else
            echo "✅  No takealot_debug.html (Takealot scraper likely succeeded)."
          fi

      # 6) Run Loot scraper
      - name: Run Loot scraper
        run: |
          source venv/bin/activate
          python scraper_loot_dom.py

      # 7) Clean up any deals older than 30 days
      - name: Clean old deals
        run: |
          source venv/bin/activate
          python cleanup.py

      # 8) Render index.html
      - name: Render HTML
        run: |
          source venv/bin/activate
          python render_deals.py

      # 9) Commit & push index.html only if it changed (never push deals.db)
      - name: Commit & push changes
        run: |
          git reset --hard HEAD

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Stage only index.html
          git add index.html

          if git diff --cached --quiet; then
            echo "No changes to index.html → skipping push."
            exit 0
          fi

          git commit -m "Automated daily update: $(date -u +'%Y-%m-%d %H:%M UTC')"
          git pull --rebase origin main
          git push origin main

      # 10) If Takealot found zero deals, upload takealot_debug.html for inspection
      - name: Upload debug HTML
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: takealot-debug
          path: takealot_debug.html



