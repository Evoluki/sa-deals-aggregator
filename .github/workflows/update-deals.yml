# File: .github/workflows/update-deals.yml
name: Daily Deals Update

on:
  push:
    branches:
      - main
  schedule:
    # Run at 04:00 UTC daily (~06:00 SAST)
    - cron: '0 4 * * *'

jobs:
  update:
    runs-on: ubuntu-latest

    steps:
      # 1) Checkout entire repo (full history)
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      # 2) Use Python 3.12 explicitly
      - name: Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      # 3) Install system dependencies (so that lxml, Playwright, etc. build properly)
      - name: Install Ubuntu packages
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-dev libxslt1-dev python3-dev
          sudo apt-get install -y \
            libnss3 libnspr4 libatk1.0-0 libatk-bridge2.0-0 \
            libcups2 libdrm2 libxkbcommon0 libxcomposite1 \
            libxdamage1 libxfixes3 libxrandr2 libgbm1 libasound2

      # 4) Create a virtual environment and install Python dependencies
      - name: Create venv & install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate

          pip install --upgrade pip
          pip install wheel

          # Now install everything from requirements.txt
          pip install -r requirements.txt

          # Finally install Playwright browsers + dependencies
          playwright install chromium
          playwright install-deps chromium

      # 5) Run the Takealot scraper (collect debug HTML if zero deals)
      - name: Run Takealot scraper
        run: |
          source venv/bin/activate
          python scraper_takealot_sqlite.py || true

          if [ -f takealot_debug.html ]; then
            echo "⚠️  takealot_debug.html exists for inspection."
          else
            echo "✅  Takealot scraper ran without generating debug HTML."
          fi

      # 6) Run the Loot scraper
      - name: Run Loot scraper
        run: |
          source venv/bin/activate
          python scraper_loot_dom.py

      # 7) Clean up old deals (>30 days)
      - name: Clean old deals
        run: |
          source venv/bin/activate
          python cleanup.py

      # 8) Render index.html
      - name: Render HTML
        run: |
          source venv/bin/activate
          python render_deals.py

      # 9) Commit & push index.html (only if it changed)
      - name: Commit & push changes
        run: |
          git reset --hard HEAD

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Stage only index.html (never deals.db)
          git add index.html

          if git diff --cached --quiet; then
            echo "No changes to index.html → skipping push."
            exit 0
          fi

          git commit -m "Automated daily update: $(date -u +'%Y-%m-%d %H:%M UTC')"
          git pull --rebase origin main
          git push origin main

      # 10) Upload takealot_debug.html as an artifact if it exists
      - name: Upload debug HTML
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: takealot-debug
          path: takealot_debug.html

